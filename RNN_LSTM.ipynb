{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN / LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6f6oZfAvn87Q",
        "cOJh9zwbogXr",
        "5SyrvB5dow9d"
      ],
      "authorship_tag": "ABX9TyNlL3KE6vT59Eq0+aCHwjMC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bolinocroustibat/text-predictor/blob/tensorflow-2/RNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "AQvHR-8KpTC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBCMFFaSlw7M",
        "outputId": "db91690d-6f5e-45bc-fbd4-a9072a23f577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.3)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import Generator, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "!pip install unidecode\n",
        "import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load text training data\n",
        "Mont Google Drive"
      ],
      "metadata": {
        "id": "Wz2ovYsomVdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bObEqq6sJ9n",
        "outputId": "c1b19fe2-bfd4-4d07-8469-61bbc2d42b4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose dataset\n",
        "dataset_name: str = \"stupeflip\""
      ],
      "metadata": {
        "id": "8HSikwoNwLjl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Text predictors/\n",
        "\n",
        "with open(f\"data/{dataset_name}/input.txt\", \"r\") as f:\n",
        "    text: str = f.read()\n",
        "text: str = unidecode.unidecode(text)\n",
        "text: str = text.lower()\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rdbC90kmRS6",
        "outputId": "bdaa668e-ed4b-463e-ff9f-3ee217790870"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Text predictors\n",
            "[intro]\n",
            "et tressaillez d'allegresse\n",
            "car votre recompense sera grande dans le ciel...\n",
            "\n",
            "[couplet 1 : king ju]\n",
            "moi je suis rascar capac et je t'attaque avec mon mac\n",
            "la vie une chausse-trappe, pas de quartier quand les lyrics frappent\n",
            "mon sourire te glace comme un clic-clac qui grince\n",
            "le voila qui revient, mince ! tous les mardis pour te serrer la pince\n",
            "donne moi le courage d'aller bouffer tous les nuages\n",
            "ecoute mon coeur, ecoute la rage, ecoute ce texte anthropophage\n",
            "ecoute ce mec qui vote reac', e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map each character to integer"
      ],
      "metadata": {
        "id": "a9AuBRt4mdog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(text)\n",
        "vocab_size: int = len(vocab)\n",
        "vocab_to_int: dict = {l: i for i, l in enumerate(vocab)}\n",
        "int_to_vocab: dict = {i: l for i, l in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "6YhI6WGFmfne"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorize text"
      ],
      "metadata": {
        "id": "1bveqa2wm1kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded: list = [vocab_to_int[l] for l in text]\n",
        "inputs, targets = encoded, encoded[1:]"
      ],
      "metadata": {
        "id": "IGTarvp3m4it"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to generate batches"
      ],
      "metadata": {
        "id": "A4JVNB3Xnvz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(\n",
        "    inputs: list, targets: list, seq_len: int, batch_size: int, noise=0\n",
        ") -> Tuple[Generator, Generator]:\n",
        "    # Size of each chunk\n",
        "    chunk_size: int = (len(inputs) - 1) // batch_size\n",
        "    # Number of sequence per chunk\n",
        "    sequences_per_chunk: int = chunk_size // seq_len\n",
        "\n",
        "    for s in range(0, sequences_per_chunk):\n",
        "        batch_inputs = np.zeros((batch_size, seq_len))\n",
        "        batch_targets = np.zeros((batch_size, seq_len))\n",
        "        for b in range(0, batch_size):\n",
        "            fr = (b * chunk_size) + (s * seq_len)\n",
        "            to = fr + seq_len\n",
        "            batch_inputs[b] = inputs[fr:to]\n",
        "            batch_targets[b] = inputs[fr + 1 : to + 1]\n",
        "\n",
        "            if noise > 0:\n",
        "                noise_indices = np.random.choice(seq_len, size=noise)\n",
        "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
        "\n",
        "        yield batch_inputs, batch_targets"
      ],
      "metadata": {
        "id": "SbCCwAqPnzI5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the method (optional)"
      ],
      "metadata": {
        "id": "6f6oZfAvn87Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_inputs, batch_targets in generate_batches(inputs, targets, 5, 32, noise=0):\n",
        "    print(batch_inputs[0], batch_targets[0])\n",
        "    break\n",
        "\n",
        "# With adding some noise:\n",
        "for batch_inputs, batch_targets in generate_batches(inputs, targets, 5, 32, noise=3):\n",
        "    print(batch_inputs[0], batch_targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F8b-l-un_11",
        "outputId": "03e3f32d-e29d-4add-c5c3-a1f02a4429a9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16. 25.  2. 35. 49.] [25.  2. 35. 49. 32.]\n",
            "[16. 25.  2. 35. 13.] [25.  2. 35. 49. 32.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom OneHot encoder as model layer"
      ],
      "metadata": {
        "id": "cOJh9zwbogXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHot(tf.keras.layers.Layer):\n",
        "    def __init__(self, depth, **kwargs):\n",
        "        super(OneHot, self).__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
      ],
      "metadata": {
        "id": "ZHuuMUpMoiIN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the OneHot layer (optional)"
      ],
      "metadata": {
        "id": "5SyrvB5dow9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RnnModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(RnnModel, self).__init__()\n",
        "        # Convolutions\n",
        "        self.one_hot = OneHot(vocab_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = self.one_hot(inputs)\n",
        "        return output\n",
        "\n",
        "\n",
        "batch_inputs, batch_targets = next(\n",
        "    generate_batches(inputs=inputs, targets=targets, seq_len=50, batch_size=32)\n",
        ")\n",
        "\n",
        "model = RnnModel(vocab_size)\n",
        "output = model.predict(batch_inputs)\n",
        "\n",
        "print(f\"Shape of the output of the model: {output.shape}\")\n",
        "\n",
        "print(\"Input letter is: {} ('{}')\".format(batch_inputs[0][0], int_to_vocab[batch_inputs[0][0]]))\n",
        "print(\"One hot representation of the letter: {}\".format(output[0][0]))\n",
        "\n",
        "# assert(output[int(batch_inputs[0][0])]==1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrkwscMvovzz",
        "outputId": "d7483d27-972b-40e1-9100-d5516bd6dc43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the output of the model: (32, 50, 68)\n",
            "Input letter is: 29.0 (l)\n",
            "One hot representation of the letter: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the model"
      ],
      "metadata": {
        "id": "7TlzWCaFpIv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the input of the model\n",
        "tf_inputs = tf.keras.Input(shape=(None,), batch_size=64)\n",
        "# Convert each value of the  input into a one encoding vector\n",
        "one_hot = OneHot(vocab_size)(tf_inputs)\n",
        "# Stack LSTM cells\n",
        "# rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot)\n",
        "# rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
        "# Create the outputs of the model\n",
        "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(one_hot)\n",
        "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "### Setup the model\n",
        "model = tf.keras.Model(inputs=tf_inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "qYs4P1HyprrY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if we can reset the RNN cells"
      ],
      "metadata": {
        "id": "kuMjchT4p0t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by resetting the cells of the RNN\n",
        "model.reset_states()\n",
        "\n",
        "# Get one batch\n",
        "batch_inputs, batch_targets = next(\n",
        "    generate_batches(inputs=inputs, targets=targets, seq_len=50, batch_size=64)\n",
        ")\n",
        "# logger.debug(f\"Shape of the inputs: {batch_inputs.shape}\")\n",
        "\n",
        "# Make a first prediction\n",
        "outputs = model.predict(batch_inputs)\n",
        "first_prediction = outputs[0][0]\n",
        "\n",
        "# Reset the states of the RNN states\n",
        "model.reset_states()\n",
        "\n",
        "# Make an other prediction to check the difference\n",
        "outputs = model.predict(batch_inputs)\n",
        "second_prediction = outputs[0][0]\n",
        "\n",
        "# Check if both prediction are equal\n",
        "assert set(first_prediction) == set(second_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcBJu1Mgp4th",
        "outputId": "6c69a37a-109d-4456-dfcc-b168b9c77f01"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (64, None) for input KerasTensor(type_spec=TensorSpec(shape=(64, None), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (32, 50).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the loss and objectives"
      ],
      "metadata": {
        "id": "b07AJHz8qw1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "X1SW3on3qyve"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set some metrics to track the progress of the training"
      ],
      "metadata": {
        "id": "4xMlkJjYq07E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "## Accuracy\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "metadata": {
        "id": "c4tNSindq-12"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the train method and the predict method in graph mode"
      ],
      "metadata": {
        "id": "WyrPizZ0rH3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Make a prediction on all the batches\n",
        "        predictions = model(inputs)\n",
        "        # Get the error/loss on these predictions\n",
        "        loss = loss_object(targets, predictions)\n",
        "    # Compute the gradient which respect to the loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    # Change the weights of the model\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
        "    train_loss(loss)\n",
        "    train_accuracy(targets, predictions)\n",
        "\n",
        "@tf.function\n",
        "def predict(inputs):\n",
        "    # Make a prediction on all the batches\n",
        "    predictions = model(inputs)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "9a-Og9-hrInr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "5C4WBwWmrTMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_states()\n",
        "\n",
        "for epoch in range(4000):\n",
        "    for batch_inputs, batch_targets in generate_batches(\n",
        "        inputs=inputs, targets=targets, seq_len=50, batch_size=64, noise=13\n",
        "    ):\n",
        "        train_step(batch_inputs, batch_targets)\n",
        "    template = \"\\r Epoch {}, Train Loss: {}, Train Accuracy: {}\"\n",
        "    print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
        "    model.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HOb1-c0rOug",
        "outputId": "b327492a-6fa7-4376-9fc9-c8ea686e711e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 611, Train Loss: 2.627392292022705, Train Accuracy: 22.973453521728516"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "Kdv9LHvQrW3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(f\"model_{dataset_name}_rnn.h5\")\n",
        "\n",
        "with open(f\"model/model_{dataset_name}_rnn_vocab_to_int\", \"w\") as f:\n",
        "    f.write(json.dumps(vocab_to_int))\n",
        "with open(f\"model/model_{dataset_name}_rnn_int_to_vocab\", \"w\") as f:\n",
        "    f.write(json.dumps(int_to_vocab))"
      ],
      "metadata": {
        "id": "aPMuJeV_rZym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate some text"
      ],
      "metadata": {
        "id": "N765gjblrgDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_states()\n",
        "\n",
        "size_poetries = 300\n",
        "\n",
        "poetries = np.zeros((64, size_poetries, 1))\n",
        "sequences = np.zeros((64, 100))\n",
        "for b in range(64):\n",
        "    rd = np.random.randint(0, len(inputs) - 100)\n",
        "    sequences[b] = inputs[rd : rd + 100]\n",
        "\n",
        "for i in range(size_poetries + 1):\n",
        "    if i > 0:\n",
        "        poetries[:, i - 1, :] = sequences\n",
        "    softmax = predict(sequences)\n",
        "    # Set the next sequences\n",
        "    sequences = np.zeros((64, 1))\n",
        "    for b in range(64):\n",
        "        argsort = np.argsort(softmax[b][0])\n",
        "        argsort = argsort[::-1]\n",
        "        # Select one of the strongest 4 proposals\n",
        "        sequences[b] = argsort[0]\n",
        "\n",
        "for b in range(64):\n",
        "    sentence: str = \"\".join([int_to_vocab[i[0]] for i in poetries[b]])\n",
        "    print(sentence)\n",
        "    with open(f\"data/{dataset_name}/output.txt\", \"w\") as outfile:\n",
        "        outfile.write(json.dumps(int_to_vocab))"
      ],
      "metadata": {
        "id": "2D4L60drriRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}