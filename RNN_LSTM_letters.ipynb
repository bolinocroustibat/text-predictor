{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bolinocroustibat/text-predictor/blob/main/RNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQvHR-8KpTC6"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBCMFFaSlw7M",
        "outputId": "32b08a40-ca61-4b71-d52d-c0eccfaeb132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.3)\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import random\n",
        "from typing import Generator, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz2ovYsomVdX"
      },
      "source": [
        "# Load text training data\n",
        "Mont Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bObEqq6sJ9n",
        "outputId": "6e18b200-3c72-4383-bc9f-d554b2935c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8HSikwoNwLjl"
      },
      "outputs": [],
      "source": [
        "# Choose dataset\n",
        "dataset_name: str = \"shakespeare\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rdbC90kmRS6",
        "outputId": "9bf2fe92-36a2-4f1d-cd04-bc16628decfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Text predictors\n",
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you know caius marcius is chief enemy to the people.\n",
            "\n",
            "all:\n",
            "we know't, we know't.\n",
            "\n",
            "first citizen:\n",
            "let us kill him, and we'll have corn at our own price.\n",
            "is't a verdict?\n",
            "\n",
            "all:\n",
            "no more talking on't; let it be done: away, away!\n",
            "\n",
            "second citizen:\n",
            "one word, good citizens.\n",
            "\n",
            "first citizen:\n",
            "we are accounted poor\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Text predictors/\n",
        "\n",
        "with open(f\"data/{dataset_name}/input.txt\", \"r\") as f:\n",
        "    text: str = f.read()\n",
        "text: str = text.lower()\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9AuBRt4mdog"
      },
      "source": [
        "# Map each character to integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6YhI6WGFmfne"
      },
      "outputs": [],
      "source": [
        "vocab = set(text)\n",
        "vocab_size: int = len(vocab)\n",
        "vocab_to_int: dict = {l: i for i, l in enumerate(vocab)}\n",
        "int_to_vocab: dict = {i: l for i, l in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bveqa2wm1kV"
      },
      "source": [
        "# Vectorize text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IGTarvp3m4it"
      },
      "outputs": [],
      "source": [
        "encoded: list = [vocab_to_int[l] for l in text]\n",
        "inputs, targets = encoded, encoded[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4JVNB3Xnvz-"
      },
      "source": [
        "# Function to generate batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SbCCwAqPnzI5"
      },
      "outputs": [],
      "source": [
        "def generate_batches(\n",
        "    inputs: list, targets: list, seq_len: int, batch_size: int, noise=0\n",
        ") -> Tuple[Generator, Generator]:\n",
        "    # Size of each chunk\n",
        "    chunk_size: int = (len(inputs) - 1) // batch_size\n",
        "    # Number of sequence per chunk\n",
        "    sequences_per_chunk: int = chunk_size // seq_len\n",
        "\n",
        "    for s in range(0, sequences_per_chunk):\n",
        "        batch_inputs = np.zeros((batch_size, seq_len))\n",
        "        batch_targets = np.zeros((batch_size, seq_len))\n",
        "        for b in range(0, batch_size):\n",
        "            fr = (b * chunk_size) + (s * seq_len)\n",
        "            to = fr + seq_len\n",
        "            batch_inputs[b] = inputs[fr:to]\n",
        "            batch_targets[b] = inputs[fr + 1 : to + 1]\n",
        "\n",
        "            if noise > 0:\n",
        "                noise_indices = np.random.choice(seq_len, size=noise)\n",
        "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
        "\n",
        "        yield batch_inputs, batch_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f6oZfAvn87Q"
      },
      "source": [
        "# Test the method (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F8b-l-un_11",
        "outputId": "b978fbf5-fb72-4bd1-f71d-3d6975d14d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1. 38.  6. 28. 20.] [38.  6. 28. 20. 17.]\n",
            "[ 1. 38.  6. 34. 34.] [38.  6. 28. 20. 17.]\n"
          ]
        }
      ],
      "source": [
        "for batch_inputs, batch_targets in generate_batches(inputs, targets, 5, 32, noise=0):\n",
        "    print(batch_inputs[0], batch_targets[0])\n",
        "    break\n",
        "\n",
        "# With adding some noise:\n",
        "for batch_inputs, batch_targets in generate_batches(inputs, targets, 5, 32, noise=3):\n",
        "    print(batch_inputs[0], batch_targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJh9zwbogXr"
      },
      "source": [
        "# Custom OneHot encoder as model layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZHuuMUpMoiIN"
      },
      "outputs": [],
      "source": [
        "class OneHot(tf.keras.layers.Layer):\n",
        "    def __init__(self, depth, **kwargs):\n",
        "        super(OneHot, self).__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SyrvB5dow9d"
      },
      "source": [
        "# Test the OneHot layer (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrkwscMvovzz",
        "outputId": "98b9c2cb-2c09-4611-8926-9cb62df9fa7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the output of the model: (32, 50, 40)\n",
            "Input letter is: 1.0 ('f')\n",
            "One hot representation of the letter: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "class RnnModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(RnnModel, self).__init__()\n",
        "        # Convolutions\n",
        "        self.one_hot = OneHot(vocab_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = self.one_hot(inputs)\n",
        "        return output\n",
        "\n",
        "\n",
        "batch_inputs, batch_targets = next(\n",
        "    generate_batches(inputs=inputs, targets=targets, seq_len=50, batch_size=32)\n",
        ")\n",
        "\n",
        "model = RnnModel(vocab_size)\n",
        "output = model.predict(batch_inputs)\n",
        "\n",
        "print(f\"Shape of the output of the model: {output.shape}\")\n",
        "\n",
        "print(\"Input letter is: {} ('{}')\".format(batch_inputs[0][0], int_to_vocab[batch_inputs[0][0]]))\n",
        "print(\"One hot representation of the letter: {}\".format(output[0][0]))\n",
        "\n",
        "# assert(output[int(batch_inputs[0][0])]==1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TlzWCaFpIv1"
      },
      "source": [
        "# Set up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qYs4P1HyprrY"
      },
      "outputs": [],
      "source": [
        "# Set the input of the model\n",
        "tf_inputs = tf.keras.Input(shape=(None,), batch_size=64)\n",
        "# Convert each value of the  input into a one encoding vector\n",
        "one_hot = OneHot(vocab_size)(tf_inputs)\n",
        "# Stack LSTM cells\n",
        "rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot)\n",
        "rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
        "# Create the outputs of the model\n",
        "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(one_hot)\n",
        "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "### Setup the model\n",
        "model = tf.keras.Model(inputs=tf_inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuMjchT4p0t4"
      },
      "source": [
        "# Check if we can reset the RNN cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcBJu1Mgp4th",
        "outputId": "13699569-8371-4fb6-a723-4b5ad0542292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (64, None) for input KerasTensor(type_spec=TensorSpec(shape=(64, None), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (32, 50).\n"
          ]
        }
      ],
      "source": [
        "# Start by resetting the cells of the RNN\n",
        "model.reset_states()\n",
        "\n",
        "# Get one batch\n",
        "batch_inputs, batch_targets = next(\n",
        "    generate_batches(inputs=inputs, targets=targets, seq_len=50, batch_size=64)\n",
        ")\n",
        "# logger.debug(f\"Shape of the inputs: {batch_inputs.shape}\")\n",
        "\n",
        "# Make a first prediction\n",
        "outputs = model.predict(batch_inputs)\n",
        "first_prediction = outputs[0][0]\n",
        "\n",
        "# Reset the states of the RNN states\n",
        "model.reset_states()\n",
        "\n",
        "# Make another prediction to check the difference\n",
        "outputs = model.predict(batch_inputs)\n",
        "second_prediction = outputs[0][0]\n",
        "\n",
        "# Check if both prediction are equal\n",
        "assert set(first_prediction) == set(second_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07AJHz8qw1Q"
      },
      "source": [
        "# Set the loss and objectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X1SW3on3qyve"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xMlkJjYq07E"
      },
      "source": [
        "# Set some metrics to track the progress of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c4tNSindq-12"
      },
      "outputs": [],
      "source": [
        "## Loss\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "## Accuracy\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyrPizZ0rH3-"
      },
      "source": [
        "# Set the train method and the predict method in graph mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9a-Og9-hrInr"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Make a prediction on all the batches\n",
        "        predictions = model(inputs)\n",
        "        # Get the error/loss on these predictions\n",
        "        loss = loss_object(targets, predictions)\n",
        "    # Compute the gradient which respect to the loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    # Change the weights of the model\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
        "    train_loss(loss)\n",
        "    train_accuracy(targets, predictions)\n",
        "\n",
        "@tf.function\n",
        "def predict(inputs):\n",
        "    # Make a prediction on all the batches\n",
        "    predictions = model(inputs)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4WBwWmrTMo"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HOb1-c0rOug",
        "outputId": "62f72074-c8db-43d4-fdb2-3648f5878e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25, Train Loss: 2.7065539360046387, Train Accuracy: 22.33930206298828"
          ]
        }
      ],
      "source": [
        "model.reset_states()\n",
        "\n",
        "for epoch in range(4000):\n",
        "    for batch_inputs, batch_targets in generate_batches(\n",
        "        inputs=inputs, targets=targets, seq_len=50, batch_size=64, noise=13\n",
        "    ):\n",
        "        train_step(batch_inputs, batch_targets)\n",
        "    template = \"\\r Epoch {}, Train Loss: {}, Train Accuracy: {}\"\n",
        "    print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
        "    model.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdv9LHvQrW3v"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPMuJeV_rZym"
      },
      "outputs": [],
      "source": [
        "model.save(f\"model_{dataset_name}_rnn.h5\")\n",
        "\n",
        "with open(f\"model/model_{dataset_name}_rnn_vocab_to_int\", \"w\") as f:\n",
        "    f.write(json.dumps(vocab_to_int))\n",
        "with open(f\"model/model_{dataset_name}_rnn_int_to_vocab\", \"w\") as f:\n",
        "    f.write(json.dumps(int_to_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N765gjblrgDJ"
      },
      "source": [
        "# Generate some text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPAjrvhu1wO7"
      },
      "outputs": [],
      "source": [
        "out_filename: str = \"output_{}\".format(datetime.now().strftime('%Y-%m-%d_%H-%M'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D4L60drriRm"
      },
      "outputs": [],
      "source": [
        "model.reset_states()\n",
        "\n",
        "size_poetries = 300\n",
        "\n",
        "poetries = np.zeros((64, size_poetries, 1))\n",
        "sequences = np.zeros((64, 100))\n",
        "for b in range(64):\n",
        "    rd = np.random.randint(0, len(inputs) - 100)\n",
        "    sequences[b] = inputs[rd : rd + 100]\n",
        "\n",
        "for i in range(size_poetries + 1):\n",
        "    if i > 0:\n",
        "        poetries[:, i - 1, :] = sequences\n",
        "    softmax = predict(sequences)\n",
        "    # Set the next sequences\n",
        "    sequences = np.zeros((64, 1))\n",
        "    for b in range(64):\n",
        "        argsort = np.argsort(softmax[b][0])\n",
        "        argsort = argsort[::-1]\n",
        "        # Select one of the strongest 4 proposals\n",
        "        sequences[b] = argsort[0]\n",
        "\n",
        "for b in range(64):\n",
        "    sentence: str = \"\".join([int_to_vocab[i[0]] for i in poetries[b]])\n",
        "    print(sentence)\n",
        "    with open(f\"data/{dataset_name}/{out_filename}.txt\", \"w\") as outfile:\n",
        "        outfile.write(sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPBVaRC0NkpxpM2TdrwiIBU",
      "collapsed_sections": [
        "6f6oZfAvn87Q",
        "cOJh9zwbogXr",
        "5SyrvB5dow9d"
      ],
      "include_colab_link": true,
      "name": "RNN / LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
